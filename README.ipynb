{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a71e32b6",
      "metadata": {
        "id": "a71e32b6"
      },
      "source": [
        "#TriviaQA Trivia Question Answering\n",
        "\n",
        "###Goal:\n",
        "The goal of this project is to train a deep residual neural network to accurately answer trivia questions using the TriviaQA dataset. By leveraging advanced techniques in natural language processing and deep learning, the model aims to overcome the challenges posed by the complex and compositional semantics of questions, complex reasoning requiring the combination of information from multiple sentences, and the difficulty in accurately extracting relevant information from the text.\n",
        "\n",
        "###Dataset:\n",
        "*   The only datasource we used for this project is the TriviaQA dataset found here: https://nlp.cs.washington.edu/triviaqa/\n",
        "*   This dataset contains over 650k question-answer-evidence triples in json format.\n",
        "*   It is separated into web evidence and wikipedia evidence categories and within those the data contained is the question, answer along with aliases of the answer and normalized versions of each, and finally search results relating to the question.\n",
        "*   The most important parts of this data are the questions, normalized answer value for target training, and normalized answer aliases for judging accuracy.\n",
        "\n",
        "###NLP Task:\n",
        "The primary NLP task in this project is to train a deep residual neural network on the TriviaQA dataset to perform question answering. Given a trivia question and the corresponding evidence from multiple sentences, the model will be trained to accurately predict the answer.\n",
        "\n",
        "###Challenges:\n",
        "The challenges in this task arise from the nature of trivia questions and the need for the model to reason and comprehend complex information effectively. These challenges include:\n",
        "\n",
        "###Complex and Compositional Semantics of Questions:\n",
        "Trivia questions often involve intricate language patterns, such as negation, comparison, or inference, which require the model to understand and interpret them correctly.\n",
        "Complex Reasoning: Answering trivia questions often involves combining information from multiple sentences and performing various types of reasoning, such as deduction, induction, or analogy. The model needs to learn how to reason effectively and make accurate predictions.\n",
        "Accurate Information Extraction: Trivia questions typically have supporting evidence spread across multiple sentences or documents. The model must accurately extract and utilize the relevant information to answer the question correctly.\n",
        "\n",
        "###Neural Methodology\n",
        "The neural methodology employed in this project comprises several key layers and techniques, including:\n",
        "\n",
        "\n",
        "1.   Embedding Layer: This layer converts the input text into distributed dense vector representations (embeddings), capturing the semantic meaning of each word or token.\n",
        "2.   Encoder Layer: An encoding layer, such as a deep residual neural network or a Transformer-based model, processes the embedded input to capture the contextual information and extract relevant features from the text.\n",
        "3.   Attention Layer: The attention mechanism allows the model to focus on different parts of the question and evidence, weighing their importance and facilitating effective reasoning and information fusion.\n",
        "4.   Attention Layer: The attention mechanism allows the model to focus on different parts of the question and evidence, weighing their importance and facilitating effective reasoning and information fusion.\n",
        "5.   Evidence Processing Layer: This layer processes the evidence based on the attention weights, aggregating the relevant information and discarding noise or irrelevant details. It aims to distill the most salient features necessary for accurate answer prediction.\n",
        "6.   Answer Prediction Layer: The answer prediction layer, typically implemented as a fully connected network followed by a softmax activation, predicts the most probable answer based on the processed evidence. It enables multi-class classification to select the correct answer from a set of options.\n",
        "\n",
        "###Baselines and Evaluation\n",
        "Two baselines are commonly used for evaluating the performance of the model:\n",
        "Human Answer Accuracy: The accuracy achieved by humans when answering the trivia questions in the dataset serves as a baseline for the model's performance. It represents the upper limit of achievable accuracy.\n",
        "Existing Models Accuracy: The accuracy of pre-existing models or state-of-the-art approaches on the TriviaQA dataset serves as another baseline. It provides a benchmark for comparing the performance of the newly trained model.\n",
        "The performance metric used for evaluation is accuracy, which measures the percentage of correctly answered trivia questions.\n",
        "\n",
        "###Usage and Workflow\n",
        "\n",
        "####To replicate and extend this project, follow these steps:\n",
        "\n",
        "1.   Acquire the TriviaQA dataset, ensuring it includes the question-answer pairs and supporting evidence.\n",
        "2.   Preprocess the dataset by tokenizing the text, converting words to embeddings, and creating input-output pairs suitable for training the model.\n",
        "3.   Implement the neural architecture by incorporating the embedding, encoding, attention, evidence processing, and answer prediction layers.\n",
        "4.   Train the model using the prepared dataset, optimizing it with backpropagation and an appropriate optimization algorithm.\n",
        "5.   Evaluate the trained model's accuracy using"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0346c8df",
      "metadata": {
        "id": "0346c8df"
      },
      "source": [
        "----------------------------------------------------\n",
        "###Data Cleaning:\n",
        "\n",
        "1.   Import Dependencies: We begin by importing the necessary libraries, such as pandas, for data manipulation and processing.\n",
        "\n",
        "2.   Read the Input Data: We read the input data from a JSON file using the appropriate function, such as pd.read_json().\n",
        "\n",
        "3.   Remove Unnecessary Columns: Since the input data contains columns that are not relevant to the task or analysis, we remove them using the drop() function.\n",
        "\n",
        "4.   Define Column Names: We create a list of column names that you want to assign to the new DataFrame.\n",
        "\n",
        "5.   Data Extraction and Transformation: Iterate over the rows of the input data using a loop, typically with iterrows(). Extract the relevant data from each row and append it to a temporary list. We customized the extraction logic based on the structure of your data.\n",
        "\n",
        "6.   Create the New DataFrame: Create a new DataFrame using the temporary list of extracted data and the defined column names. Used the pd.DataFrame() function, passing the list as the data argument and the column names as the columns argument.\n",
        "\n",
        "7.   Save the DataFrame to CSV: Save the newly created DataFrame as a CSV file for further use or analysis. Utilize the to_csv() function, specifying the desired output file path and setting index=False to exclude the index column in the CSV file.\n",
        "\n",
        "8.   After executing this code, you should have a CSV file named \"QA_DATA_2.csv\" containing the cleaned and preprocessed data, ready for further use or loading into a different application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b21ca80",
      "metadata": {
        "id": "1b21ca80"
      },
      "source": [
        "-------------------------------------------------\n",
        "\n",
        "###Data Preprocessing\n",
        "\n",
        "1.   Traversing a Directory and Reading Files: We utilized the os module to traverse through a directory. It iterates over the files in the directory using os.walk(). For each file, it reads the contents using open() and stores the text in a list called dataset. Each entry in the list contains the text and the label, which is obtained from the base name of the directory.\n",
        "\n",
        "2.   Saving the Dataset to JSON: The dataset list is then saved as a JSON file (output_file) using the json.dump() function. This step creates a JSON file that contains the text and label information for each file in the directory.\n",
        "\n",
        "3.   Loading a CSV Dataset into a DataFrame: The code loads a CSV file (csv_file_path) into a pandas DataFrame using the pd.read_csv() function. This step shows that the CSV file contains columns named 'question', 'answer', and 'description'.\n",
        "\n",
        "4.   Accessing DataFrame Columns: After loading the CSV file, the code assigns the 'question', 'answer', and 'description' columns of the DataFrame to separate variables (questions, answers, descriptions) for further processing.\n",
        "\n",
        "5.   Handling Missing Values: We used the fillna() method to replace any missing values in the 'answer' column with the string 'No answer'. This step ensures that there are no empty or NaN values in the 'answer' column.\n",
        "\n",
        "6.   The provided code combines file traversal, JSON file creation, loading CSV data, and data cleaning steps to process and prepare the data for further analysis or model training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fb7317",
      "metadata": {
        "id": "a3fb7317"
      },
      "source": [
        "----------------------------------------------------------------\n",
        "###Data Preprocessing using pre trained models layers:\n",
        "\n",
        "The models used in this project include a simple neural network using the Keras Sequential API and GloVe embeddings. Each of these pretrained models has its own data preprocessing layers. Here are the steps involved in the preprocessing layers for these models:\n",
        "\n",
        "1.   Lowercasing Answers: This step involves converting the answers in the training, validation, and test sets to lowercase. Lowercasing is performed to normalize the text and reduce the vocabulary size. It ensures consistency in the text representation, as uppercase and lowercase versions of the same word are treated as the same.\n",
        "\n",
        "2.   Mapping Labels to Integers: In order to train a classification model, textual answers need to be converted into a numerical format. This is done by mapping each unique answer to a unique integer value. For example, if there are 5 distinct answers in the dataset, they can be mapped to the integers 0 to 4.\n",
        "\n",
        "3.   Encoding Labels: Once the answers are mapped to integers, they are further encoded to represent them as one-hot vectors or categorical labels. This step is necessary for classification tasks, where the model needs to predict a single class among multiple options.\n",
        "\n",
        "4.   Tokenizing Questions: The questions in the dataset are tokenized, which involves splitting them into individual words or subword units. This process converts text into numerical sequences that can be fed into the model. Each word or subword is assigned a unique numerical token.\n",
        "\n",
        "5.   Padding Sequences: To ensure consistent input shape for the model, the tokenized sequences of questions are padded with special tokens to match the maximum sequence length in the dataset. Padding ensures that all input sequences have the same length, allowing for efficient batch processing during training.\n",
        "\n",
        "These preprocessing layers are applied to the input data before passing it to the respective layers of the pretrained models. Proper data preprocessing is essential to ensure compatibility between the input data and the pretrained models. By following the specific preprocessing requirements of each pretrained model, you can achieve optimal performance and accurate predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a402565",
      "metadata": {
        "id": "7a402565"
      },
      "source": [
        "---------------------------------------------------\n",
        "\n",
        "###Keras Sequential API Model Training:\n",
        "\n",
        "Implementation of a simple neural network model using the Keras Sequential API. It includes the data preprocessing steps as well as the model training and evaluation. Here's a breakdown of the code:\n",
        "\n",
        "1.   Import Dependencies: We import the necessary libraries, including NumPy, pandas, scikit-learn, and the required modules from Keras.\n",
        "\n",
        "2.   Extract Question and Answer Columns: Retrieve the 'question' and 'answer' columns from the DataFrame and store them in separate variables.\n",
        "\n",
        "3.   Split the Dataset: Split the dataset into training, validation, and test sets using the train_test_split() function from scikit-learn. Specify the test size and random state for reproducibility.\n",
        "\n",
        "4.   Lowercasing Answers: Convert the answers to lowercase using list comprehensions to ensure consistency in text representation.\n",
        "\n",
        "5.   Create Label Mapping: Create a dictionary to map unique answers to integers. Iterate through the training answers and assign a unique integer to each answer that hasn't been encountered before. Also, create a reverse mapping from integers back to labels.\n",
        "\n",
        "6.   Encode Labels: Encode the training, validation, and test answers into numerical labels using the mapping created in the previous step. Use list comprehensions and the label-to-integer mapping to convert the answers to encoded form.\n",
        "\n",
        "7.   Tokenize Questions: Tokenize the questions using the Tokenizer class from Keras. Fit the tokenizer on the training questions and use it to convert the questions to sequences of integers.\n",
        "\n",
        "8.   Pad Sequences: Pad the sequences of question tokens to ensure they have the same length. Determine the maximum sequence length among all the sequences and use the pad_sequences() function to pad/truncate the sequences accordingly.\n",
        "\n",
        "9.   Define Model Architecture: Build the model architecture using the Sequential API. Add an embedding layer, a flatten layer, a dense layer with ReLU activation, and a final dense layer with softmax activation for multi-class classification.\n",
        "\n",
        "10.   Compile the Model: Compile the model by specifying the loss function, optimizer, and evaluation metric. In this case, the loss function is 'sparse_categorical_crossentropy' since the labels are integers.\n",
        "\n",
        "11.   Train the Model: Train the model on the training data and validate it on the validation data. Specify the number of epochs and the batch size.\n",
        "\n",
        "12.   Evaluate the Model: Evaluate the trained model on the test data using the evaluate() method. Retrieve the loss and accuracy values and print them to the console."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c65bcd2f",
      "metadata": {
        "id": "c65bcd2f"
      },
      "source": [
        "--------------------------------------------------------\n",
        "###GloVe Model Training:\n",
        "\n",
        "We use the pre-trained GloVe word embeddings. It involves similar preprocessing steps Here's an explanation of steps:\n",
        "\n",
        "1.   Load GloVe Word Embeddings: Load the pre-trained GloVe word embeddings file. Iterate through each line, extract the word and the corresponding embedding vector, and store them in the embedding_dict dictionary.\n",
        "\n",
        "2.   Create an Embedding Matrix: Create an embedding matrix to initialize the embedding layer of the model. The matrix has dimensions (num_words, embedding_dim), where num_words represents the total number of unique words in the tokenizer's word index, and embedding_dim represents the dimensionality of the GloVe word embeddings. Iterate through the word index items and assign the corresponding GloVe embedding vector to the matrix if available.\n",
        "\n",
        "3.   Define the Model Architecture: Build the model architecture by adding layers to the Sequential model. The first layer is an Embedding layer initialized with the embedding matrix, which is set to non-trainable (trainable=False) to keep the pre-trained embeddings fixed. The subsequent layers are the same as before.\n",
        "\n",
        "4.   Compile the Model: Compile the model using the same configuration as before, specifying the loss function, optimizer, and evaluation metric.\n",
        "\n",
        "5.   Train the Model: Train the model on the training data and validate it on the validation data, similar to the previous code.\n",
        "\n",
        "6.   Evaluate the Model: Evaluate the trained model on the test data using the evaluate() method, as done previously. Retrieve the loss and accuracy values and print them to the console.\n",
        "\n",
        "7.   By incorporating pre-trained GloVe word embeddings, the model can benefit from the semantic information captured in the word vectors, potentially improving its performance in understanding the questions and providing accurate answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c856256",
      "metadata": {
        "id": "7c856256"
      },
      "source": [
        "------------------------------------\n",
        "###Challenges in training a trivia question answering model include:\n",
        "\n",
        "1.   Computational Resources: Adequate computational resources, such as GPUs or TPUs, are required for efficient training and inference.\n",
        "\n",
        "2.   Fine-tuning Pre-trained Models: Fine-tuning pre-trained models on domain-specific data can improve performance on specific tasks.\n",
        "\n",
        "3.   Handling Long and Complex Questions: Strategies like attention mechanisms or hierarchical modeling are needed to handle longer and more complex questions effectively.\n",
        "\n",
        "4.   Lack of Contextual Understanding: Incorporating contextual understanding techniques, such as contextual word embeddings or larger pre-trained models, can help improve understanding of the questions.\n",
        "\n",
        "5.   Ambiguity and Variability in Questions: Models need to handle ambiguous and variable questions by capturing nuances and employing advanced reasoning mechanisms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2036052",
      "metadata": {
        "id": "d2036052"
      },
      "source": [
        "------------------------------------------------------------------\n",
        "###Limitations\n",
        "\n",
        "1.   Hardware Limitations: Training deep neural networks can require significant computational resources, such as GPUs or TPUs. Limited hardware resources may restrict the model's size, training time, and overall performance.\n",
        "\n",
        "2.   Software Compatibility: Ensuring that the software stack, including libraries, frameworks, and dependencies, is compatible and properly configured can be a challenge. Mismatched versions or compatibility issues can lead to errors or suboptimal performance.\n",
        "\n",
        "3.   GPU Memory Limit: Deep learning models often require a large amount of GPU memory, especially when working with complex architectures or large batch sizes. GPU memory limitations may require adjustments to model size, batch size, or exploring alternative architectures.\n",
        "\n",
        "4.   Code Optimizations: Optimizing the code for efficiency, memory usage, and speed is crucial. This includes techniques like batch processing, data parallelism, and model parallelism to make the most of available hardware resources.\n",
        "\n",
        "5.   Debugging Challenges: Debugging neural network models can be challenging due to their complex nature. Identifying and resolving issues related to architecture, data preprocessing, loss functions, or training procedures requires a systematic approach and deep understanding of the model and its components.\n",
        "\n",
        "6.   Learning Curve: Deep learning involves a steep learning curve, especially ###for those new to the field. Understanding the underlying principles, architectures, and techniques, as well as staying up-to-date with the latest research, can be time-consuming but essential for achieving optimal results.\n",
        "\n",
        "Addressing these challenges may involve resource allocation, software configuration, memory management, code optimization techniques, and investing time in learning and staying updated with the field of deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf6f7e3",
      "metadata": {
        "id": "1cf6f7e3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}